\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Fefferman:2013}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Motivation}{1}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Hypothesis}{1}{section.2}\protected@file@percent }
\citation{Mikolov:2013}
\citation{Vaswani:2017}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Outline of the model architecture used for next merchant prediction. Representations are taken from the embedding layers.\relax }}{2}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{arch}{{1}{2}{Outline of the model architecture used for next merchant prediction. Representations are taken from the embedding layers.\relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Applications}{2}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Baseline Representations}{2}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Model Architecture}{2}{section.5}\protected@file@percent }
\citation{Devlin:2018}
\citation{Vaswani:2017}
\citation{Liu2019LatentSC}
\citation{McInnes:2018}
\@writefile{toc}{\contentsline {section}{\numberline {6}Evaluation Metrics}{3}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Recall@K}{3}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Transferability of Representations}{3}{subsection.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Relevant Literature}{3}{section.7}\protected@file@percent }
\citation{Fefferman:2013}
\citation{Bengio:2012}
\citation{Pennington:2014}
\@writefile{toc}{\contentsline {section}{\numberline {8}Experimental protocol}{4}{section.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Dataset description}{4}{subsection.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Preprocessing}{4}{subsection.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Categorical inputs}{4}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Cyclical inputs}{4}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Continuous inputs}{4}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Combination of Inputs}{4}{subsection.8.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Default values for hyperparameters explored.\relax }}{5}{table.caption.5}\protected@file@percent }
\newlabel{hparam-defaults}{{1}{5}{Default values for hyperparameters explored.\relax }{table.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4}Loss Functions}{5}{subsection.8.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5}Hyperparameters}{5}{subsection.8.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Empirical Results}{5}{section.9}\protected@file@percent }
\newlabel{training-loss-curve}{{2a}{6}{Loss\relax }{figure.caption.6}{}}
\newlabel{sub@training-loss-curve}{{a}{6}{Loss\relax }{figure.caption.6}{}}
\newlabel{training-recall-curve}{{2b}{6}{Recall@1\relax }{figure.caption.6}{}}
\newlabel{sub@training-recall-curve}{{b}{6}{Recall@1\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Learning curves on the training set. The X-axis is number of steps.\relax }}{6}{figure.caption.6}\protected@file@percent }
\newlabel{curves}{{2}{6}{Learning curves on the training set. The X-axis is number of steps.\relax }{figure.caption.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Results for the best model using only merchant name.\relax }}{6}{table.caption.7}\protected@file@percent }
\newlabel{training-table}{{2}{6}{Results for the best model using only merchant name.\relax }{table.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Training results}{6}{subsection.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.1.1}Best model}{6}{subsubsection.9.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.1.2}Model Variations}{6}{subsubsection.9.1.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Some of the hyperparameter variations evaluated. Values shown here were computed on the validation set. Lower loss is better.\relax }}{6}{table.caption.8}\protected@file@percent }
\newlabel{hparams-table}{{3}{6}{Some of the hyperparameter variations evaluated. Values shown here were computed on the validation set. Lower loss is better.\relax }{table.caption.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Some of the feature combinations evaluated. Values shown here were computed on the validation set, by a model with 4 layers given a sequence length of 32, and ran for 20 epochs. Lower loss is better.\relax }}{7}{table.caption.9}\protected@file@percent }
\newlabel{features-table}{{4}{7}{Some of the feature combinations evaluated. Values shown here were computed on the validation set, by a model with 4 layers given a sequence length of 32, and ran for 20 epochs. Lower loss is better.\relax }{table.caption.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Reference classifier trained on \texttt  {gensim} merchant representations.\relax }}{7}{table.caption.10}\protected@file@percent }
\newlabel{gensim-eval}{{5}{7}{Reference classifier trained on \texttt {gensim} merchant representations.\relax }{table.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Re-using Representations}{7}{subsection.9.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}Projections of Representation Spaces}{7}{subsection.9.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Reference classifier using our model's learned merchant representations.\relax }}{7}{table.caption.11}\protected@file@percent }
\newlabel{merchant-rep-eval}{{6}{7}{Reference classifier using our model's learned merchant representations.\relax }{table.caption.11}{}}
\citation{Gauvrit:2011}
\citation{Zenil:2012}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Reference classifier using our model's learned user representations.\relax }}{8}{table.caption.12}\protected@file@percent }
\newlabel{user-rep-eval}{{7}{8}{Reference classifier using our model's learned user representations.\relax }{table.caption.12}{}}
\newlabel{gensim-proj}{{3a}{8}{Baseline\relax }{figure.caption.13}{}}
\newlabel{sub@gensim-proj}{{a}{8}{Baseline\relax }{figure.caption.13}{}}
\newlabel{merch-rep-proj}{{3b}{8}{Learned by our model\relax }{figure.caption.13}{}}
\newlabel{sub@merch-rep-proj}{{b}{8}{Learned by our model\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Two dimensional projections of merchant representations\relax }}{8}{figure.caption.13}\protected@file@percent }
\newlabel{projections}{{3}{8}{Two dimensional projections of merchant representations\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10}Conclusion and future work}{8}{section.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Predictability vs. stochasticity}{8}{section*.14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Weighted loss}{8}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Semantic embedding}{8}{section*.16}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Explainability}{8}{section*.17}\protected@file@percent }
\bibstyle{plain}
\bibdata{refs}
\bibcite{Bengio:2012}{1}
\bibcite{Devlin:2018}{2}
\bibcite{Fefferman:2013}{3}
\bibcite{Gauvrit:2011}{4}
\bibcite{Liu2019LatentSC}{5}
\bibcite{McInnes:2018}{6}
\bibcite{Mikolov:2013}{7}
\bibcite{Pennington:2014}{8}
\bibcite{Vaswani:2017}{9}
\bibcite{Zenil:2012}{10}
\@writefile{toc}{\contentsline {section}{\numberline {11}Acknowledgement}{9}{section.11}\protected@file@percent }
